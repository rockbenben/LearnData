"use strict";(self.webpackChunklearn_data=self.webpackChunklearn_data||[]).push([[1951],{94157:(e,n,a)=>{a.r(n),a.d(n,{default:()=>X});var t=a(64304);const r=(0,t._)("p",null,"每个人的声音都是独一无二的，克隆自己的声音可以用于制作高度个性化的内容，如播客、视频、音乐等。",-1),l=(0,t._)("p",null,"你的声音是个人品牌的重要组成部分。利用人工智能，你可以不需要亲自录音就能生成大量优质音频内容，节省时间的同时确保内容质量和一致性。市场上虽有众多第三方语音生成技术，但它们大多数使用通用或他人的声音，导致内容缺乏个性化特质。例如，「注意看，这个男人叫小帅」的声音已经在众多影视作品中被重复使用。与之不同，AI 克隆技术能提供前所未有的个性化和定制体验。",-1),i=(0,t._)("p",null,"艾什莉的播客就是一个典型例子，她利用 AI 生成了根据当日新闻热点定制的讲稿，再用 AI 克隆的自己的声音进行朗读，配上背景音乐，既经济又高效。",-1),o={href:"https://github.com/Plachtaa/VITS-fast-fine-tuning",target:"_blank",rel:"noopener noreferrer"},s=(0,t._)("h2",{id:"收集语音样本",tabindex:"-1"},[(0,t._)("a",{class:"header-anchor",href:"#收集语音样本"},[(0,t._)("span",null,"收集语音样本")])],-1),p=(0,t._)("p",null,[(0,t.Uk)("克隆声音的第一步是准备自己的声音样本。确保录音中只有你的声音，且语音清晰、语速均匀。录音完成后，需检查 "),(0,t._)("code",null,"final_annotation"),(0,t.Uk)(" 等 txt 文件的音频转写情况，确认停顿和文字是否正确。")],-1),c=(0,t._)("figure",null,[(0,t._)("img",{src:"https://img.newzone.top/2023-10-08-21-53-46.png?imageMogr2/format/webp/thumbnail/400x",alt:"",tabindex:"0",loading:"lazy"}),(0,t._)("figcaption")],-1),h={href:"https://weixinxcxdb.oss-cn-beijing.aliyuncs.com/gwYinPinKu/BZNSYP.rar",target:"_blank",rel:"noopener noreferrer"},d=(0,t._)("figure",null,[(0,t._)("img",{src:"https://img.newzone.top/2023-10-11-10-22-27.png?imageMogr2/format/webp/thumbnail/400x",alt:"",tabindex:"0",loading:"lazy"}),(0,t._)("figcaption")],-1),u=(0,t._)("h2",{id:"云端训练模型",tabindex:"-1"},[(0,t._)("a",{class:"header-anchor",href:"#云端训练模型"},[(0,t._)("span",null,"云端训练模型")])],-1),g={href:"https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/README_ZH.md",target:"_blank",rel:"noopener noreferrer"},_={href:"https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing",target:"_blank",rel:"noopener noreferrer"},m=(0,t._)("code",null,"max_epochs",-1),f=(0,t._)("figure",null,[(0,t._)("img",{src:"https://img.newzone.top/2023-10-07-07-37-52.png?imageMogr2/format/webp",alt:"",tabindex:"0",loading:"lazy"}),(0,t._)("figcaption")],-1),k=(0,t._)("p",null,"我曾在 Colab 上用 8 分钟的 B 站视频进行训练，但三小时后由于超出免费额度被终止。后来我在配备了 3080Ti 的本地设备上进行训练，4 小时后便完成了调试。",-1),b=(0,t._)("h2",{id:"本地训练模型",tabindex:"-1"},[(0,t._)("a",{class:"header-anchor",href:"#本地训练模型"},[(0,t._)("span",null,"本地训练模型")])],-1),v={href:"https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/LOCAL.md",target:"_blank",rel:"noopener noreferrer"},U={href:"https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/",target:"_blank",rel:"noopener noreferrer"},y=(0,t._)("code",null,"pip install --upgrade numpy",-1),x={href:"https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/sampled_audio4ft_v2.zip",target:"_blank",rel:"noopener noreferrer"},T=(0,t._)("p",null,"第 7 步 (下载模型与配置)：",-1),w={href:"https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/D_0.pth",target:"_blank",rel:"noopener noreferrer"},I={href:"https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/G_0.pth",target:"_blank",rel:"noopener noreferrer"},z={href:"https://huggingface.co/datasets/Plachta/sampled_audio4ft/resolve/main/VITS-Chinese/config.json",target:"_blank",rel:"noopener noreferrer"},C={href:"https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/D_0-p.pth",target:"_blank",rel:"noopener noreferrer"},A={href:"https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/G_0-p.pth",target:"_blank",rel:"noopener noreferrer"},P={href:"https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai/resolve/main/model/config.json",target:"_blank",rel:"noopener noreferrer"},W={href:"https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/D_trilingual.pth",target:"_blank",rel:"noopener noreferrer"},j={href:"https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/pretrained_models/G_trilingual.pth",target:"_blank",rel:"noopener noreferrer"},D={href:"https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer/resolve/main/configs/uma_trilingual.json",target:"_blank",rel:"noopener noreferrer"},S=(0,t.uE)("<li>选择上方一种模型进行下载。完成下载后，将 G 模型重命名为 <code>G_0.pth</code>，将 D 模型重命名为 <code>D_0.pth</code>，并将配置文件 .json 重命名为 <code>finetune_speaker.json</code>。<code>G_0.pth</code> 和 <code>D_0.pth</code> 放入 pretrained_models 目录，<code>finetune_speaker.json</code> 放入 config 目录。特别注意，要保证 json 文件是直接下载而非复制粘贴，以防后续步骤中打开 inference 出现问题。</li>",1),V=(0,t.uE)('<li><p>第 8 步：由于 LOCAL.md 教程中未包含在线视频，所以需要将相关视频文件下载到本地。</p></li><li><p>第 9 步：运行以下命令：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python scripts/video2audio.py\npython scripts/denoise_audio.py\n\npython scripts/long_audio_transcribe.py <span class="token parameter variable">--languages</span> C <span class="token parameter variable">--whisper_size</span> large-v2\npython scripts/short_audio_transcribe.py <span class="token parameter variable">--languages</span> C <span class="token parameter variable">--whisper_size</span> large-v2\n\npython scripts/resample.py\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>第 10 步：执行 <code>python preprocess_v2.py --add_auxiliary_data True --languages C</code>。</p></li><li><p>第 11 步：</p><ul><li>开始训练：<code>python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs 5000 --drop_speaker_embed True</code>。</li><li>如果训练过程中断，要继续训练，执行 <code>python finetune_speaker_v2.py -m ./OUTPUT_MODEL --max_epochs 10000 --drop_speaker_embed False --cont True</code>。</li></ul><figure><img src="https://img.newzone.top/2023-10-10-05-01-08.png?imageMogr2/format/webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure></li>',4),E=(0,t._)("h2",{id:"文字生成语音",tabindex:"-1"},[(0,t._)("a",{class:"header-anchor",href:"#文字生成语音"},[(0,t._)("span",null,"文字生成语音")])],-1),L=(0,t._)("code",null,"OUTPUT_MODEL",-1),M={href:"https://github.com/Plachtaa/VITS-fast-fine-tuning/releases",target:"_blank",rel:"noopener noreferrer"},O=(0,t._)("code",null,"G_latest.pth",-1),G=(0,t._)("code",null,"finetune_speaker.json",-1),Z=(0,t._)("figure",null,[(0,t._)("img",{src:"https://img.newzone.top/2023-10-09-09-08-31.png?imageMogr2/format/webp",alt:"",tabindex:"0",loading:"lazy"}),(0,t._)("figcaption")],-1),J=(0,t._)("code",null,"finetune_speaker.json",-1),F=(0,t._)("code",null,'File "inference.py", line 99',-1),H={href:"https://wwva.lanzouq.com/iIy5m1b4bosf",target:"_blank",rel:"noopener noreferrer"},N=(0,t._)("code",null,"config.json",-1),B=(0,t._)("code",null,"finetune_speaker.json",-1),q=(0,t.uE)('<h2 id="声音微调" tabindex="-1"><a class="header-anchor" href="#声音微调"><span>声音微调</span></a></h2><h3 id="生成声音与原声无关" tabindex="-1"><a class="header-anchor" href="#生成声音与原声无关"><span>生成声音与原声无关</span></a></h3><p>在检查语料转写文本时，尽量减少对 <code>short_character_anno.txt</code> 和 <code>long_character_anno.txt</code> 文件的非必要修改。过多的修改可能会导致生成的声音与原始声音产生明显差异。最初的训练中，可以将 <code>max_epochs</code> 设置为 200。在确生成声音符合预期后，再进行后续的训练。</p><p>例如：因个人语调的不同，我读的是「假语村言」，但 whisper 将其识别为「甲乙寸言」。如果我保留 whisper 的识别结果不做修改，最终生成的声音与我本人的声音非常接近。但当我将 whisper 识别结果修改为原文本后，生成的声音产生了显著变化，几乎听不出与原声音的相似性。我们只需要对那些发音明显错误的部分做出修改。例如，「假」和「甲」的发音相同，无需更改；「语」和「乙」之间的差异较大，但由于是个人发音的特点，也无需更改；对于「村」和「寸」这样声调不同的词汇，可以在检查录音后做出相应更改。</p><h3 id="语音克隆的「口音」问题" tabindex="-1"><a class="header-anchor" href="#语音克隆的「口音」问题"><span>语音克隆的「口音」问题</span></a></h3>',5),K=(0,t.uE)('<p>这个示例是使用 8 分钟 B 站视频和 CJE 模型训练出的。但你可能注意到了明显的断调口音问题，仿佛一个日本人在说中文。正如 @zachx121 指出的，「CJE 用的时候 romaji 的注音，就好比说用汉语拼音去标注英文单词的发音一样会有“口音”」。为了避免这个问题，可以使用纯中文设计的 C 模式进行训练和生成，以确保音频的自然和准确性。增加训练次数也有助于改善口音问题。</p><h2 id="常见问题" tabindex="-1"><a class="header-anchor" href="#常见问题"><span>常见问题</span></a></h2><h3 id="无法启动-inference" tabindex="-1"><a class="header-anchor" href="#无法启动-inference"><span>无法启动 inference</span></a></h3><p>如果你遇到无法启动 inference 的问题，通常是因为 <code>finetune_speaker.json</code> 配置文件有问题。确保你下载的 json 文件是对应的版本，并且格式完整。如果问题仍然存在，可以考虑使用 <code>configs/modified_finetune_speaker.json</code> 文件替代原有配置文件，通常这样可以解决运行中出现的错误。</p><h3 id="长音频识别问题" tabindex="-1"><a class="header-anchor" href="#长音频识别问题"><span>长音频识别问题</span></a></h3><p>要注意，长音频需要采用 wav 格式。即即使原本为 mp3 格式的音频文件在后期转为 wav，也可能出错。因此，直接使用 wav 格式进行长音频录制或选择是更好的做法。</p><h3 id="录音中出现-zh" tabindex="-1"><a class="header-anchor" href="#录音中出现-zh"><span>录音中出现 zh</span></a></h3><p>在使用纯中文模式调试时，音频前后可能会标注当前语言，例如，中文语言中出现 ZH 标注。为去除这些不必要的语言标注，可以将生成语言设置为 <code>Mix</code> 模式。</p><h3 id="长句读音含糊" tabindex="-1"><a class="header-anchor" href="#长句读音含糊"><span>长句读音含糊</span></a></h3><p>用短句生成了一段 6 秒的清晰音频，但当将两个相同的短句重复形成长句时，生成的语音时长仅为 9 秒，发音特别含糊。可能是因为语料文本过度修改，一些语音没有被 Whisper 识别，但已经标注。这与训练次数无关，出现此问题后，需重新检查语料。</p>',10),R={},X=(0,a(86683).Z)(R,[["render",function(e,n){const a=(0,t.up)("VidStack"),R=(0,t.up)("ExternalLinkIcon");return(0,t.wg)(),(0,t.iD)("div",null,[r,l,i,(0,t.Wm)(a,{src:"https://oss.newzone.top/audio/clonevoice00.wav",title:"克隆音频实例"}),(0,t._)("p",null,[(0,t.Uk)("我使用了 "),(0,t._)("a",o,[(0,t.Uk)("VITS-fast-fine-tuning"),(0,t.Wm)(R)]),(0,t.Uk)(" 来克隆我的声音。这款工具能从短音频、长音频或视频中克隆特定角色的声音，只需几小时即可完成预训练的 VITS 模型的微调。微调后的模型不仅能进行声线转换，还能完成中、日、英三种语言的文本到语音的转换。")]),s,p,c,(0,t._)("p",null,[(0,t.Uk)("为增加语音样本的多样性，选择不同主题和领域的文本材料。我使用的文本来自"),(0,t._)("a",h,[(0,t.Uk)("标贝数据集"),(0,t.Wm)(R)]),(0,t.Uk)("，该数据集包含 10000 条文本和对应的读音。我选了 300 条用于短音频录制，每条录音时长在 2 至 10 秒之间。语料的质量优于数量，如果需要，可以减少语料条数或使用长视频。VITS-fast-fine-tuning 工具会自动将长音频剪切成短音频。")]),d,u,(0,t._)("p",null,[(0,t.Uk)("关于模型的微调和部署，你可以参考官方 GitHub "),(0,t._)("a",g,[(0,t.Uk)("页面"),(0,t.Wm)(R)]),(0,t.Uk)("的详细操作指南。")]),(0,t._)("p",null,[(0,t.Uk)("在 "),(0,t._)("a",_,[(0,t.Uk)("Google Colab"),(0,t.Wm)(R)]),(0,t.Uk)(" 进行模型微调时，可能会因长时间未连接或超出免费配置限制而中断。为防止数据丢失，应提前选择「STEP 5 下载模型」的下载选项。在 Colab 进行云端训练时，建议长音频时长控制在 20 分钟以内，"),m,(0,t.Uk)(" 设置为 100。如需进一步提升模型质量，可继续训练模型，再进行 100 次 epochs。")]),f,k,b,(0,t._)("p",null,[(0,t.Uk)("如果需要进行深入的模型调整，比如执行 5000 次 epochs，可能需要数天的时间。为此，你可以参考 "),(0,t._)("a",v,[(0,t.Uk)("LOCAL.md"),(0,t.Wm)(R)]),(0,t.Uk)(" 来在本地环境进行训练。而针对其中可能存在的不明确部分，以下补充具体步骤和建议。")]),(0,t._)("ul",null,[(0,t._)("li",null,[(0,t._)("p",null,[(0,t.Uk)("第 0 步：预先确认本地环境的 Python 版本为 3.8，并且已经安装了 "),(0,t._)("a",U,[(0,t.Uk)("Microsoft C++ 生成工具"),(0,t.Wm)(R)]),(0,t.Uk)(" 和 ffmpeg。这样可以预防潜在错误。在启动本地运行之前，执行 "),y,(0,t.Uk)(" 来更新 numpy 版本。")])]),(0,t._)("li",null,[(0,t._)("p",null,[(0,t.Uk)("第 6 步：鉴于 wget 下载命令在 Windows 中可能不起作用，建议手动下载 "),(0,t._)("a",x,[(0,t.Uk)("sampled_audio4ft_v2.zip"),(0,t.Wm)(R)]),(0,t.Uk)("，随后将文件解压至运行路径。")])]),(0,t._)("li",null,[T,(0,t._)("ul",null,[(0,t._)("li",null,[(0,t.Uk)("C 模型（纯中文）：下载 HuggingFace 平台上的 "),(0,t._)("a",w,[(0,t.Uk)("D_0.pth"),(0,t.Wm)(R)]),(0,t.Uk)("、"),(0,t._)("a",I,[(0,t.Uk)("G_0.pth"),(0,t.Wm)(R)]),(0,t.Uk)(" 和 "),(0,t._)("a",z,[(0,t.Uk)("config.json"),(0,t.Wm)(R)]),(0,t.Uk)("。")]),(0,t._)("li",null,[(0,t.Uk)("CJ 模型（中日）：下载 "),(0,t._)("a",C,[(0,t.Uk)("D_0-p.pth"),(0,t.Wm)(R)]),(0,t.Uk)("、"),(0,t._)("a",A,[(0,t.Uk)("G_0-p.pth"),(0,t.Wm)(R)]),(0,t.Uk)(" 和 "),(0,t._)("a",P,[(0,t.Uk)("config.json"),(0,t.Wm)(R)]),(0,t.Uk)("。")]),(0,t._)("li",null,[(0,t.Uk)("CJE 模型（中日英）：下载 "),(0,t._)("a",W,[(0,t.Uk)("D_trilingual.pth"),(0,t.Wm)(R)]),(0,t.Uk)("、"),(0,t._)("a",j,[(0,t.Uk)("G_trilingual.pth"),(0,t.Wm)(R)]),(0,t.Uk)(" 和 "),(0,t._)("a",D,[(0,t.Uk)("uma_trilingual.json"),(0,t.Wm)(R)]),(0,t.Uk)("。")]),S])]),V]),E,(0,t._)("p",null,[(0,t.Uk)("微调完成后，将 "),L,(0,t.Uk)(" 下的模型文件和 config 文件放在语音生成工具 "),(0,t._)("a",M,[(0,t.Uk)("inference"),(0,t.Wm)(R)]),(0,t.Uk)(" 解压文件夹下，其文件名分别为 "),O,(0,t.Uk)(" 和 "),G,(0,t.Uk)("。一切准备就绪后，运行 inference.exe, 浏览器会自动弹出窗口，即可在本地环境下生成个性化的语音内容。注意其所在路径不能有中文字符或者空格。")]),Z,(0,t._)("p",null,[(0,t.Uk)("在这个阶段，特别注意中文模型（即 languages C）的 "),J,(0,t.Uk)(" 格式问题。确保「speaks」部分被修改为字典格式，否则在运行 inference 时，你可能会遇到 "),F,(0,t.Uk)(" 的报错。为方便，你可以直接点击"),(0,t._)("a",H,[(0,t.Uk)("这里"),(0,t.Wm)(R)]),(0,t.Uk)("下载我调整好的 json 文件。如果你不需要二次元声音，可以直接使用 OUTPUT_MODEL 下的 "),N,(0,t.Uk)(" 替代 "),B,(0,t.Uk)("。")]),q,(0,t.Wm)(a,{src:"https://oss.newzone.top/audio/clonevoice01.wav",title:"克隆音频实例 2"}),K])}]])},86683:(e,n)=>{n.Z=(e,n)=>{const a=e.__vccOpts||e;for(const[e,t]of n)a[e]=t;return a}},64351:(e,n,a)=>{a.r(n),a.d(n,{data:()=>t});const t=JSON.parse('{"key":"v-72653828","path":"/posts/2023-10-07-clone-voice.html","title":"别再被同质化的内容淹没！用 AI 克隆技术打造你独特的声音品牌！","lang":"zh-CN","frontmatter":{"title":"别再被同质化的内容淹没！用 AI 克隆技术打造你独特的声音品牌！","date":"2023-10-07T00:00:00.000Z","category":["工具"],"tag":["AI","VITS","声音克隆"],"order":-56,"description":"每个人的声音都是独一无二的，克隆自己的声音可以用于制作高度个性化的内容，如播客、视频、音乐等。 你的声音是个人品牌的重要组成部分。利用人工智能，你可以不需要亲自录音就能生成大量优质音频内容，节省时间的同时确保内容质量和一致性。市场上虽有众多第三方语音生成技术，但它们大多数使用通用或他人的声音，导致内容缺乏个性化特质。例如，「注意看，这个男人叫小帅」的声音已经在众多影视作品中被重复使用。与之不同，AI 克隆技术能提供前所未有的个性化和定制体验。 艾什莉的播客就是一个典型例子，她利用 AI 生成了根据当日新闻热点定制的讲稿，再用 AI 克隆的自己的声音进行朗读，配上背景音乐，既经济又高效。","head":[["meta",{"property":"og:url","content":"https://newzone.top/posts/2023-10-07-clone-voice.html"}],["meta",{"property":"og:site_name","content":"LearnData-开源笔记"}],["meta",{"property":"og:title","content":"别再被同质化的内容淹没！用 AI 克隆技术打造你独特的声音品牌！"}],["meta",{"property":"og:description","content":"每个人的声音都是独一无二的，克隆自己的声音可以用于制作高度个性化的内容，如播客、视频、音乐等。 你的声音是个人品牌的重要组成部分。利用人工智能，你可以不需要亲自录音就能生成大量优质音频内容，节省时间的同时确保内容质量和一致性。市场上虽有众多第三方语音生成技术，但它们大多数使用通用或他人的声音，导致内容缺乏个性化特质。例如，「注意看，这个男人叫小帅」的声音已经在众多影视作品中被重复使用。与之不同，AI 克隆技术能提供前所未有的个性化和定制体验。 艾什莉的播客就是一个典型例子，她利用 AI 生成了根据当日新闻热点定制的讲稿，再用 AI 克隆的自己的声音进行朗读，配上背景音乐，既经济又高效。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-12-18T04:53:57.000Z"}],["meta",{"property":"article:author","content":"清顺"}],["meta",{"property":"article:tag","content":"AI"}],["meta",{"property":"article:tag","content":"VITS"}],["meta",{"property":"article:tag","content":"声音克隆"}],["meta",{"property":"article:published_time","content":"2023-10-07T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-12-18T04:53:57.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"别再被同质化的内容淹没！用 AI 克隆技术打造你独特的声音品牌！\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-10-07T00:00:00.000Z\\",\\"dateModified\\":\\"2023-12-18T04:53:57.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"清顺\\",\\"url\\":\\"https://newzone.top\\"}]}"]]},"headers":[{"level":2,"title":"收集语音样本","slug":"收集语音样本","link":"#收集语音样本","children":[]},{"level":2,"title":"云端训练模型","slug":"云端训练模型","link":"#云端训练模型","children":[]},{"level":2,"title":"本地训练模型","slug":"本地训练模型","link":"#本地训练模型","children":[]},{"level":2,"title":"文字生成语音","slug":"文字生成语音","link":"#文字生成语音","children":[]},{"level":2,"title":"声音微调","slug":"声音微调","link":"#声音微调","children":[{"level":3,"title":"生成声音与原声无关","slug":"生成声音与原声无关","link":"#生成声音与原声无关","children":[]},{"level":3,"title":"语音克隆的「口音」问题","slug":"语音克隆的「口音」问题","link":"#语音克隆的「口音」问题","children":[]}]},{"level":2,"title":"常见问题","slug":"常见问题","link":"#常见问题","children":[{"level":3,"title":"无法启动 inference","slug":"无法启动-inference","link":"#无法启动-inference","children":[]},{"level":3,"title":"长音频识别问题","slug":"长音频识别问题","link":"#长音频识别问题","children":[]},{"level":3,"title":"录音中出现 zh","slug":"录音中出现-zh","link":"#录音中出现-zh","children":[]},{"level":3,"title":"长句读音含糊","slug":"长句读音含糊","link":"#长句读音含糊","children":[]}]}],"git":{"createdTime":1696991517000,"updatedTime":1702875237000,"contributors":[{"name":"rockbenben","email":"qingwhat@gmail.com","commits":7}]},"readingTime":{"minutes":7.93,"words":2379},"filePathRelative":"_posts/2023-10-07-clone-voice.md","localizedDate":"2023年10月7日","excerpt":"<p>每个人的声音都是独一无二的，克隆自己的声音可以用于制作高度个性化的内容，如播客、视频、音乐等。</p>\\n<p>你的声音是个人品牌的重要组成部分。利用人工智能，你可以不需要亲自录音就能生成大量优质音频内容，节省时间的同时确保内容质量和一致性。市场上虽有众多第三方语音生成技术，但它们大多数使用通用或他人的声音，导致内容缺乏个性化特质。例如，「注意看，这个男人叫小帅」的声音已经在众多影视作品中被重复使用。与之不同，AI 克隆技术能提供前所未有的个性化和定制体验。</p>\\n<p>艾什莉的播客就是一个典型例子，她利用 AI 生成了根据当日新闻热点定制的讲稿，再用 AI 克隆的自己的声音进行朗读，配上背景音乐，既经济又高效。</p>","autoDesc":true}')}}]);